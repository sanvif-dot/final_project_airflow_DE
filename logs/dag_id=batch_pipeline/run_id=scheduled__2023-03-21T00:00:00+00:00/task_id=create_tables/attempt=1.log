[2023-03-22T01:20:55.350+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T01:20:55.379+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T01:20:55.380+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T01:20:55.384+0000] {taskinstance.py:1283} INFO - Starting attempt 1 of 1
[2023-03-22T01:20:55.386+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T01:20:55.427+0000] {taskinstance.py:1303} INFO - Executing <Task(PostgresOperator): create_tables> on 2023-03-21 00:00:00+00:00
[2023-03-22T01:20:55.443+0000] {standard_task_runner.py:55} INFO - Started process 69 to run task
[2023-03-22T01:20:55.460+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'create_tables', 'scheduled__2023-03-21T00:00:00+00:00', '--job-id', '172', '--raw', '--subdir', 'DAGS_FOLDER/load_csv.py', '--cfg-path', '/tmp/tmp46ill002']
[2023-03-22T01:20:55.463+0000] {standard_task_runner.py:83} INFO - Job 172: Subtask create_tables
[2023-03-22T01:20:55.639+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-22T01:20:55.698+0000] {abstractoperator.py:615} ERROR - Exception rendering Jinja template for task 'create_tables', field 'sql'. Template: 'create_tables.sql'
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/abstractoperator.py", line 607, in _do_render_template_fields
    seen_oids,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/abstractoperator.py", line 659, in render_template
    template = jinja_env.get_template(value)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/environment.py", line 1010, in get_template
    return self._load_template(name, globals)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/environment.py", line 969, in _load_template
    template = self.loader.load(self, name, self.make_globals(globals))
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/loaders.py", line 126, in load
    source, filename, uptodate = self.get_source(environment, name)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/loaders.py", line 218, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: create_tables.sql
[2023-03-22T01:20:55.703+0000] {taskinstance.py:1775} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1377, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1495, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 2122, in render_templates
    original_task.render_template_fields(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1179, in render_template_fields
    self._do_render_template_fields(self, self.template_fields, context, jinja_env, set())
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/abstractoperator.py", line 607, in _do_render_template_fields
    seen_oids,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/abstractoperator.py", line 659, in render_template
    template = jinja_env.get_template(value)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/environment.py", line 1010, in get_template
    return self._load_template(name, globals)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/environment.py", line 969, in _load_template
    template = self.loader.load(self, name, self.make_globals(globals))
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/loaders.py", line 126, in load
    source, filename, uptodate = self.get_source(environment, name)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/loaders.py", line 218, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: create_tables.sql
[2023-03-22T01:20:55.722+0000] {taskinstance.py:1326} INFO - Marking task as FAILED. dag_id=batch_pipeline, task_id=create_tables, execution_date=20230321T000000, start_date=20230322T012055, end_date=20230322T012055
[2023-03-22T01:20:55.746+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 172 for task create_tables (create_tables.sql; 69)
[2023-03-22T01:20:55.769+0000] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-03-22T01:20:55.811+0000] {taskinstance.py:2585} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-22T02:28:39.748+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:28:39.804+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:28:39.807+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:28:39.810+0000] {taskinstance.py:1283} INFO - Starting attempt 1 of 1
[2023-03-22T02:28:39.812+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:28:39.913+0000] {taskinstance.py:1303} INFO - Executing <Task(PostgresOperator): create_tables> on 2023-03-21 00:00:00+00:00
[2023-03-22T02:28:39.977+0000] {standard_task_runner.py:55} INFO - Started process 187 to run task
[2023-03-22T02:28:39.999+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'create_tables', 'scheduled__2023-03-21T00:00:00+00:00', '--job-id', '184', '--raw', '--subdir', 'DAGS_FOLDER/load_csv.py', '--cfg-path', '/tmp/tmpc_e7miy7']
[2023-03-22T02:28:40.021+0000] {standard_task_runner.py:83} INFO - Job 184: Subtask create_tables
[2023-03-22T02:28:40.466+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-22T02:28:40.682+0000] {abstractoperator.py:615} ERROR - Exception rendering Jinja template for task 'create_tables', field 'sql'. Template: 'create_tables.sql'
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/abstractoperator.py", line 607, in _do_render_template_fields
    seen_oids,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/abstractoperator.py", line 659, in render_template
    template = jinja_env.get_template(value)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/environment.py", line 1010, in get_template
    return self._load_template(name, globals)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/environment.py", line 969, in _load_template
    template = self.loader.load(self, name, self.make_globals(globals))
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/loaders.py", line 126, in load
    source, filename, uptodate = self.get_source(environment, name)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/loaders.py", line 218, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: create_tables.sql
[2023-03-22T02:28:40.704+0000] {taskinstance.py:1775} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1377, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1495, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 2122, in render_templates
    original_task.render_template_fields(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1179, in render_template_fields
    self._do_render_template_fields(self, self.template_fields, context, jinja_env, set())
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/abstractoperator.py", line 607, in _do_render_template_fields
    seen_oids,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/abstractoperator.py", line 659, in render_template
    template = jinja_env.get_template(value)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/environment.py", line 1010, in get_template
    return self._load_template(name, globals)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/environment.py", line 969, in _load_template
    template = self.loader.load(self, name, self.make_globals(globals))
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/loaders.py", line 126, in load
    source, filename, uptodate = self.get_source(environment, name)
  File "/home/airflow/.local/lib/python3.7/site-packages/jinja2/loaders.py", line 218, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: create_tables.sql
[2023-03-22T02:28:40.772+0000] {taskinstance.py:1326} INFO - Marking task as FAILED. dag_id=batch_pipeline, task_id=create_tables, execution_date=20230321T000000, start_date=20230322T022839, end_date=20230322T022840
[2023-03-22T02:28:40.882+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 184 for task create_tables (create_tables.sql; 187)
[2023-03-22T02:28:41.048+0000] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-03-22T02:28:41.199+0000] {taskinstance.py:2585} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-22T02:30:39.651+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:30:39.714+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:30:39.716+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:30:39.718+0000] {taskinstance.py:1283} INFO - Starting attempt 1 of 1
[2023-03-22T02:30:39.720+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:30:39.872+0000] {taskinstance.py:1303} INFO - Executing <Task(PostgresOperator): create_tables> on 2023-03-21 00:00:00+00:00
[2023-03-22T02:30:39.910+0000] {standard_task_runner.py:55} INFO - Started process 228 to run task
[2023-03-22T02:30:39.947+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'create_tables', 'scheduled__2023-03-21T00:00:00+00:00', '--job-id', '187', '--raw', '--subdir', 'DAGS_FOLDER/batch_pipeline.py', '--cfg-path', '/tmp/tmp1me01h8g']
[2023-03-22T02:30:39.952+0000] {standard_task_runner.py:83} INFO - Job 187: Subtask create_tables
[2023-03-22T02:30:40.439+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-22T02:30:40.942+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=create_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-21T00:00:00+00:00
[2023-03-22T02:30:40.969+0000] {sql.py:254} INFO - Executing: CREATE TABLE IF NOT EXISTS orders (
    order_id smallint NOT NULL,
    customer_id bpchar,
    employee_id smallint,
    order_date date,
    required_date date,
    shipped_date date,
    ship_via smallint,
    freight real,
    ship_name character varying(40),
    ship_address character varying(60),
    ship_city character varying(15),
    ship_region character varying(15),
    ship_postal_code character varying(10),
    ship_country character varying(15),

    CONSTRAINT pk_orders PRIMARY KEY (order_id)
);

CREATE TABLE IF NOT EXISTS order_details (
    order_id int REFERENCES orders (order_id),
    product_id int,
    unit_price real,
    quantity int,
    discount real
);
[2023-03-22T02:30:41.090+0000] {taskinstance.py:1775} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/common/sql/operators/sql.py", line 255, in execute
    hook = self.get_db_hook()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/common/sql/operators/sql.py", line 179, in get_db_hook
    return self._hook
  File "/home/airflow/.local/lib/python3.7/site-packages/cached_property.py", line 36, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/common/sql/operators/sql.py", line 141, in _hook
    conn = BaseHook.get_connection(self.conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/hooks/base.py", line 72, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 435, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `conn_postgres_postgres` isn't defined
[2023-03-22T02:30:41.126+0000] {taskinstance.py:1326} INFO - Marking task as FAILED. dag_id=batch_pipeline, task_id=create_tables, execution_date=20230321T000000, start_date=20230322T023039, end_date=20230322T023041
[2023-03-22T02:30:41.180+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 187 for task create_tables (The conn_id `conn_postgres_postgres` isn't defined; 228)
[2023-03-22T02:30:41.300+0000] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-03-22T02:30:41.428+0000] {taskinstance.py:2585} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-22T02:36:27.134+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:36:27.195+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:36:27.201+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:36:27.203+0000] {taskinstance.py:1283} INFO - Starting attempt 1 of 1
[2023-03-22T02:36:27.205+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:36:27.298+0000] {taskinstance.py:1303} INFO - Executing <Task(PostgresOperator): create_tables> on 2023-03-21 00:00:00+00:00
[2023-03-22T02:36:27.328+0000] {standard_task_runner.py:55} INFO - Started process 327 to run task
[2023-03-22T02:36:27.386+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'create_tables', 'scheduled__2023-03-21T00:00:00+00:00', '--job-id', '190', '--raw', '--subdir', 'DAGS_FOLDER/batch_pipeline.py', '--cfg-path', '/tmp/tmpx7mqqehp']
[2023-03-22T02:36:27.395+0000] {standard_task_runner.py:83} INFO - Job 190: Subtask create_tables
[2023-03-22T02:36:27.960+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-22T02:36:28.333+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=create_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-21T00:00:00+00:00
[2023-03-22T02:36:28.355+0000] {sql.py:254} INFO - Executing: CREATE TABLE IF NOT EXISTS orders (
    order_id smallint NOT NULL,
    customer_id bpchar,
    employee_id smallint,
    order_date date,
    required_date date,
    shipped_date date,
    ship_via smallint,
    freight real,
    ship_name character varying(40),
    ship_address character varying(60),
    ship_city character varying(15),
    ship_region character varying(15),
    ship_postal_code character varying(10),
    ship_country character varying(15),

    CONSTRAINT pk_orders PRIMARY KEY (order_id)
);

CREATE TABLE IF NOT EXISTS order_details (
    order_id int REFERENCES orders (order_id),
    product_id int,
    unit_price real,
    quantity int,
    discount real
);
[2023-03-22T02:36:28.403+0000] {taskinstance.py:1775} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/common/sql/operators/sql.py", line 255, in execute
    hook = self.get_db_hook()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/common/sql/operators/sql.py", line 179, in get_db_hook
    return self._hook
  File "/home/airflow/.local/lib/python3.7/site-packages/cached_property.py", line 36, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/common/sql/operators/sql.py", line 141, in _hook
    conn = BaseHook.get_connection(self.conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/hooks/base.py", line 72, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 435, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `conn_postgres_southwind` isn't defined
[2023-03-22T02:36:28.414+0000] {taskinstance.py:1326} INFO - Marking task as FAILED. dag_id=batch_pipeline, task_id=create_tables, execution_date=20230321T000000, start_date=20230322T023627, end_date=20230322T023628
[2023-03-22T02:36:28.490+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 190 for task create_tables (The conn_id `conn_postgres_southwind` isn't defined; 327)
[2023-03-22T02:36:28.552+0000] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-03-22T02:36:28.611+0000] {taskinstance.py:2585} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-22T02:43:52.342+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:43:52.367+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:43:52.369+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:43:52.372+0000] {taskinstance.py:1283} INFO - Starting attempt 1 of 1
[2023-03-22T02:43:52.375+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:43:52.426+0000] {taskinstance.py:1303} INFO - Executing <Task(PostgresOperator): create_tables> on 2023-03-21 00:00:00+00:00
[2023-03-22T02:43:52.443+0000] {standard_task_runner.py:55} INFO - Started process 455 to run task
[2023-03-22T02:43:52.460+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'create_tables', 'scheduled__2023-03-21T00:00:00+00:00', '--job-id', '193', '--raw', '--subdir', 'DAGS_FOLDER/batch_pipeline.py', '--cfg-path', '/tmp/tmpgkg5zhvo']
[2023-03-22T02:43:52.463+0000] {standard_task_runner.py:83} INFO - Job 193: Subtask create_tables
[2023-03-22T02:43:52.611+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.create_tables scheduled__2023-03-21T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-22T02:43:52.779+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=create_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-21T00:00:00+00:00
[2023-03-22T02:43:52.789+0000] {sql.py:254} INFO - Executing: CREATE TABLE IF NOT EXISTS orders (
    order_id smallint NOT NULL,
    customer_id bpchar,
    employee_id smallint,
    order_date date,
    required_date date,
    shipped_date date,
    ship_via smallint,
    freight real,
    ship_name character varying(40),
    ship_address character varying(60),
    ship_city character varying(15),
    ship_region character varying(15),
    ship_postal_code character varying(10),
    ship_country character varying(15),

    CONSTRAINT pk_orders PRIMARY KEY (order_id)
);

CREATE TABLE IF NOT EXISTS order_details (
    order_id int REFERENCES orders (order_id),
    product_id int,
    unit_price real,
    quantity int,
    discount real
);
[2023-03-22T02:43:52.865+0000] {taskinstance.py:1775} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/common/sql/operators/sql.py", line 255, in execute
    hook = self.get_db_hook()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/common/sql/operators/sql.py", line 179, in get_db_hook
    return self._hook
  File "/home/airflow/.local/lib/python3.7/site-packages/cached_property.py", line 36, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/common/sql/operators/sql.py", line 141, in _hook
    conn = BaseHook.get_connection(self.conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/hooks/base.py", line 72, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 435, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `conn_postgres_***` isn't defined
[2023-03-22T02:43:52.879+0000] {taskinstance.py:1326} INFO - Marking task as FAILED. dag_id=batch_pipeline, task_id=create_tables, execution_date=20230321T000000, start_date=20230322T024352, end_date=20230322T024352
[2023-03-22T02:43:52.940+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 193 for task create_tables (The conn_id `conn_postgres_***` isn't defined; 455)
[2023-03-22T02:43:52.987+0000] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-03-22T02:43:53.052+0000] {taskinstance.py:2585} INFO - 0 downstream tasks scheduled from follow-on schedule check
