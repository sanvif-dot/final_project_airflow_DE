[2023-03-22T01:20:52.618+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T01:20:52.640+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T01:20:52.641+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T01:20:52.643+0000] {taskinstance.py:1283} INFO - Starting attempt 1 of 1
[2023-03-22T01:20:52.645+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T01:20:52.679+0000] {taskinstance.py:1303} INFO - Executing <Task(PythonOperator): extract_database_tables> on 2023-03-21 00:00:00+00:00
[2023-03-22T01:20:52.694+0000] {standard_task_runner.py:55} INFO - Started process 65 to run task
[2023-03-22T01:20:52.710+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'extract_database_tables', 'scheduled__2023-03-21T00:00:00+00:00', '--job-id', '171', '--raw', '--subdir', 'DAGS_FOLDER/load_csv.py', '--cfg-path', '/tmp/tmp8u6v5r76']
[2023-03-22T01:20:52.715+0000] {standard_task_runner.py:83} INFO - Job 171: Subtask extract_database_tables
[2023-03-22T01:20:52.876+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-22T01:20:53.070+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=extract_database_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-21T00:00:00+00:00
[2023-03-22T01:20:53.236+0000] {logging_mixin.py:137} INFO - Erro: (psycopg2.OperationalError) connection to server at "host.docker.internal" (192.168.65.2), port 5432 failed: FATAL:  password authentication failed for user "postgres"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-03-22T01:20:53.239+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-03-22T01:20:53.273+0000] {taskinstance.py:1326} INFO - Marking task as SUCCESS. dag_id=batch_pipeline, task_id=extract_database_tables, execution_date=20230321T000000, start_date=20230322T012052, end_date=20230322T012053
[2023-03-22T01:20:53.373+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-03-22T01:20:53.427+0000] {taskinstance.py:2585} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-03-22T02:28:35.810+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:28:35.830+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:28:35.832+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:28:35.834+0000] {taskinstance.py:1283} INFO - Starting attempt 1 of 1
[2023-03-22T02:28:35.836+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:28:35.866+0000] {taskinstance.py:1303} INFO - Executing <Task(PythonOperator): extract_database_tables> on 2023-03-21 00:00:00+00:00
[2023-03-22T02:28:35.877+0000] {standard_task_runner.py:55} INFO - Started process 178 to run task
[2023-03-22T02:28:35.886+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'extract_database_tables', 'scheduled__2023-03-21T00:00:00+00:00', '--job-id', '183', '--raw', '--subdir', 'DAGS_FOLDER/load_csv.py', '--cfg-path', '/tmp/tmp1wccq34z']
[2023-03-22T02:28:35.891+0000] {standard_task_runner.py:83} INFO - Job 183: Subtask extract_database_tables
[2023-03-22T02:28:36.014+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-22T02:28:36.143+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Sanvi
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=extract_database_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-21T00:00:00+00:00
[2023-03-22T02:28:36.168+0000] {logging_mixin.py:137} INFO - Erro: (psycopg2.errors.UndefinedTable) relation "precipitation" does not exist
LINE 1: select * from precipitation;
                      ^

[SQL: select * from precipitation;]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2023-03-22T02:28:36.170+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-03-22T02:28:36.189+0000] {taskinstance.py:1326} INFO - Marking task as SUCCESS. dag_id=batch_pipeline, task_id=extract_database_tables, execution_date=20230321T000000, start_date=20230322T022835, end_date=20230322T022836
[2023-03-22T02:28:36.263+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-03-22T02:28:36.318+0000] {taskinstance.py:2585} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-03-22T02:30:31.756+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:30:31.792+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:30:31.833+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:30:31.908+0000] {taskinstance.py:1283} INFO - Starting attempt 1 of 1
[2023-03-22T02:30:31.911+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:30:31.974+0000] {taskinstance.py:1303} INFO - Executing <Task(PythonOperator): extract_database_tables> on 2023-03-21 00:00:00+00:00
[2023-03-22T02:30:31.987+0000] {standard_task_runner.py:55} INFO - Started process 217 to run task
[2023-03-22T02:30:31.998+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'extract_database_tables', 'scheduled__2023-03-21T00:00:00+00:00', '--job-id', '186', '--raw', '--subdir', 'DAGS_FOLDER/batch_pipeline.py', '--cfg-path', '/tmp/tmphvo1ov0a']
[2023-03-22T02:30:32.002+0000] {standard_task_runner.py:83} INFO - Job 186: Subtask extract_database_tables
[2023-03-22T02:30:32.134+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-22T02:30:32.396+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=extract_database_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-21T00:00:00+00:00
[2023-03-22T02:30:32.435+0000] {logging_mixin.py:137} INFO - Erro: (psycopg2.errors.UndefinedTable) relation "categories" does not exist
LINE 1: select * from categories;
                      ^

[SQL: select * from categories;]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2023-03-22T02:30:32.438+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-03-22T02:30:32.474+0000] {taskinstance.py:1326} INFO - Marking task as SUCCESS. dag_id=batch_pipeline, task_id=extract_database_tables, execution_date=20230321T000000, start_date=20230322T023031, end_date=20230322T023032
[2023-03-22T02:30:32.540+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-03-22T02:30:32.603+0000] {taskinstance.py:2585} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-03-22T02:36:16.128+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:36:16.231+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:36:16.235+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:36:16.238+0000] {taskinstance.py:1283} INFO - Starting attempt 1 of 1
[2023-03-22T02:36:16.240+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:36:16.343+0000] {taskinstance.py:1303} INFO - Executing <Task(PythonOperator): extract_database_tables> on 2023-03-21 00:00:00+00:00
[2023-03-22T02:36:16.372+0000] {standard_task_runner.py:55} INFO - Started process 324 to run task
[2023-03-22T02:36:16.417+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'extract_database_tables', 'scheduled__2023-03-21T00:00:00+00:00', '--job-id', '189', '--raw', '--subdir', 'DAGS_FOLDER/batch_pipeline.py', '--cfg-path', '/tmp/tmpl0bie_vm']
[2023-03-22T02:36:16.420+0000] {standard_task_runner.py:83} INFO - Job 189: Subtask extract_database_tables
[2023-03-22T02:36:16.796+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-22T02:36:17.259+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=extract_database_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-21T00:00:00+00:00
[2023-03-22T02:36:17.653+0000] {logging_mixin.py:137} INFO - Erro: (psycopg2.OperationalError) connection to server at "host.docker.internal" (192.168.65.2), port 5432 failed: FATAL:  password authentication failed for user "postgres"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-03-22T02:36:17.659+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-03-22T02:36:17.731+0000] {taskinstance.py:1326} INFO - Marking task as SUCCESS. dag_id=batch_pipeline, task_id=extract_database_tables, execution_date=20230321T000000, start_date=20230322T023616, end_date=20230322T023617
[2023-03-22T02:36:17.895+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-03-22T02:36:18.211+0000] {taskinstance.py:2585} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-03-22T02:43:48.617+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:43:48.643+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [queued]>
[2023-03-22T02:43:48.645+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:43:48.648+0000] {taskinstance.py:1283} INFO - Starting attempt 1 of 1
[2023-03-22T02:43:48.650+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-22T02:43:48.701+0000] {taskinstance.py:1303} INFO - Executing <Task(PythonOperator): extract_database_tables> on 2023-03-21 00:00:00+00:00
[2023-03-22T02:43:48.717+0000] {standard_task_runner.py:55} INFO - Started process 452 to run task
[2023-03-22T02:43:48.738+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'extract_database_tables', 'scheduled__2023-03-21T00:00:00+00:00', '--job-id', '192', '--raw', '--subdir', 'DAGS_FOLDER/batch_pipeline.py', '--cfg-path', '/tmp/tmp6c9wsde7']
[2023-03-22T02:43:48.743+0000] {standard_task_runner.py:83} INFO - Job 192: Subtask extract_database_tables
[2023-03-22T02:43:48.967+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-21T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-22T02:43:49.177+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=extract_database_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-21T00:00:00+00:00
[2023-03-22T02:43:49.211+0000] {logging_mixin.py:137} INFO - Erro: (psycopg2.errors.UndefinedTable) relation "categories" does not exist
LINE 1: select * from categories;
                      ^

[SQL: select * from categories;]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2023-03-22T02:43:49.219+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-03-22T02:43:49.262+0000] {taskinstance.py:1326} INFO - Marking task as SUCCESS. dag_id=batch_pipeline, task_id=extract_database_tables, execution_date=20230321T000000, start_date=20230322T024348, end_date=20230322T024349
[2023-03-22T02:43:49.356+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-03-22T02:43:49.460+0000] {taskinstance.py:2585} INFO - 1 downstream tasks scheduled from follow-on schedule check
