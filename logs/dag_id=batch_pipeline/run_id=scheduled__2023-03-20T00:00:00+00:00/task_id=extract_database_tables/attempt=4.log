[2023-03-21T13:16:46.645+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [queued]>
[2023-03-21T13:16:46.715+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [queued]>
[2023-03-21T13:16:46.718+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-21T13:16:46.723+0000] {taskinstance.py:1283} INFO - Starting attempt 4 of 4
[2023-03-21T13:16:46.725+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-21T13:16:46.829+0000] {taskinstance.py:1303} INFO - Executing <Task(PythonOperator): extract_database_tables> on 2023-03-20 00:00:00+00:00
[2023-03-21T13:16:46.893+0000] {standard_task_runner.py:55} INFO - Started process 976 to run task
[2023-03-21T13:16:46.936+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'extract_database_tables', 'scheduled__2023-03-20T00:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/batch_pipeline.py', '--cfg-path', '/tmp/tmp28lh0v4v']
[2023-03-21T13:16:46.941+0000] {standard_task_runner.py:83} INFO - Job 29: Subtask extract_database_tables
[2023-03-21T13:16:47.375+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-21T13:16:47.852+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=extract_database_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=4
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-20T00:00:00+00:00
[2023-03-21T13:16:47.924+0000] {logging_mixin.py:137} INFO - Erro: (psycopg2.OperationalError) connection to server at "host.docker.internal" (192.168.65.2), port 5432 failed: FATAL:  password authentication failed for user "postgres"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-03-21T13:16:47.927+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-03-21T13:16:47.975+0000] {taskinstance.py:1326} INFO - Marking task as SUCCESS. dag_id=batch_pipeline, task_id=extract_database_tables, execution_date=20230320T000000, start_date=20230321T131646, end_date=20230321T131647
[2023-03-21T13:16:48.114+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-03-21T13:16:48.357+0000] {taskinstance.py:2585} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-03-21T15:00:58.854+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [queued]>
[2023-03-21T15:00:58.877+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [queued]>
[2023-03-21T15:00:58.886+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-21T15:00:58.889+0000] {taskinstance.py:1283} INFO - Starting attempt 4 of 4
[2023-03-21T15:00:58.891+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-21T15:00:58.940+0000] {taskinstance.py:1303} INFO - Executing <Task(PythonOperator): extract_database_tables> on 2023-03-20 00:00:00+00:00
[2023-03-21T15:00:58.958+0000] {standard_task_runner.py:55} INFO - Started process 2882 to run task
[2023-03-21T15:00:58.974+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'extract_database_tables', 'scheduled__2023-03-20T00:00:00+00:00', '--job-id', '105', '--raw', '--subdir', 'DAGS_FOLDER/batch_pipeline.py', '--cfg-path', '/tmp/tmp1glfz3hq']
[2023-03-21T15:00:58.979+0000] {standard_task_runner.py:83} INFO - Job 105: Subtask extract_database_tables
[2023-03-21T15:00:59.146+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-21T15:00:59.345+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=extract_database_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=4
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-20T00:00:00+00:00
[2023-03-21T15:00:59.362+0000] {logging_mixin.py:137} INFO - Erro: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-03-21T15:00:59.365+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-03-21T15:00:59.397+0000] {taskinstance.py:1326} INFO - Marking task as SUCCESS. dag_id=batch_pipeline, task_id=extract_database_tables, execution_date=20230320T000000, start_date=20230321T150058, end_date=20230321T150059
[2023-03-21T15:00:59.492+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-03-21T15:00:59.618+0000] {taskinstance.py:2585} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-03-21T15:18:05.584+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [queued]>
[2023-03-21T15:18:05.645+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [queued]>
[2023-03-21T15:18:05.646+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-21T15:18:05.649+0000] {taskinstance.py:1283} INFO - Starting attempt 4 of 4
[2023-03-21T15:18:05.652+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-21T15:18:05.733+0000] {taskinstance.py:1303} INFO - Executing <Task(PythonOperator): extract_database_tables> on 2023-03-20 00:00:00+00:00
[2023-03-21T15:18:05.774+0000] {standard_task_runner.py:55} INFO - Started process 3193 to run task
[2023-03-21T15:18:05.835+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'extract_database_tables', 'scheduled__2023-03-20T00:00:00+00:00', '--job-id', '120', '--raw', '--subdir', 'DAGS_FOLDER/load_csv.py', '--cfg-path', '/tmp/tmpjoi6r57j']
[2023-03-21T15:18:05.838+0000] {standard_task_runner.py:83} INFO - Job 120: Subtask extract_database_tables
[2023-03-21T15:18:06.352+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-21T15:18:06.952+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=extract_database_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=4
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-20T00:00:00+00:00
[2023-03-21T15:18:07.012+0000] {logging_mixin.py:137} INFO - Erro: (psycopg2.OperationalError) connection to server at "host.docker.internal" (192.168.65.2), port 5432 failed: FATAL:  password authentication failed for user "postgres"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-03-21T15:18:07.015+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-03-21T15:18:07.061+0000] {taskinstance.py:1326} INFO - Marking task as SUCCESS. dag_id=batch_pipeline, task_id=extract_database_tables, execution_date=20230320T000000, start_date=20230321T151805, end_date=20230321T151807
[2023-03-21T15:18:07.143+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-03-21T15:18:07.278+0000] {taskinstance.py:2585} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-03-21T15:34:27.168+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [queued]>
[2023-03-21T15:34:27.256+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [queued]>
[2023-03-21T15:34:27.258+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-21T15:34:27.265+0000] {taskinstance.py:1283} INFO - Starting attempt 4 of 4
[2023-03-21T15:34:27.267+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-21T15:34:27.385+0000] {taskinstance.py:1303} INFO - Executing <Task(PythonOperator): extract_database_tables> on 2023-03-20 00:00:00+00:00
[2023-03-21T15:34:27.415+0000] {standard_task_runner.py:55} INFO - Started process 3487 to run task
[2023-03-21T15:34:27.462+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'extract_database_tables', 'scheduled__2023-03-20T00:00:00+00:00', '--job-id', '132', '--raw', '--subdir', 'DAGS_FOLDER/batch_pipeline.py', '--cfg-path', '/tmp/tmpqb9scm8_']
[2023-03-21T15:34:27.464+0000] {standard_task_runner.py:83} INFO - Job 132: Subtask extract_database_tables
[2023-03-21T15:34:28.147+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-21T15:34:28.823+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=extract_database_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=4
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-20T00:00:00+00:00
[2023-03-21T15:34:28.853+0000] {logging_mixin.py:137} INFO - Erro: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-03-21T15:34:28.856+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-03-21T15:34:28.952+0000] {taskinstance.py:1326} INFO - Marking task as SUCCESS. dag_id=batch_pipeline, task_id=extract_database_tables, execution_date=20230320T000000, start_date=20230321T153427, end_date=20230321T153428
[2023-03-21T15:34:29.176+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-03-21T15:34:29.388+0000] {taskinstance.py:2585} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-03-21T15:53:07.688+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [queued]>
[2023-03-21T15:53:07.716+0000] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [queued]>
[2023-03-21T15:53:07.721+0000] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-03-21T15:53:07.723+0000] {taskinstance.py:1283} INFO - Starting attempt 4 of 4
[2023-03-21T15:53:07.727+0000] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-03-21T15:53:07.792+0000] {taskinstance.py:1303} INFO - Executing <Task(PythonOperator): extract_database_tables> on 2023-03-20 00:00:00+00:00
[2023-03-21T15:53:07.825+0000] {standard_task_runner.py:55} INFO - Started process 3838 to run task
[2023-03-21T15:53:07.888+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'batch_pipeline', 'extract_database_tables', 'scheduled__2023-03-20T00:00:00+00:00', '--job-id', '156', '--raw', '--subdir', 'DAGS_FOLDER/load_csv.py', '--cfg-path', '/tmp/tmp3npiabkr']
[2023-03-21T15:53:07.891+0000] {standard_task_runner.py:83} INFO - Job 156: Subtask extract_database_tables
[2023-03-21T15:53:08.431+0000] {task_command.py:388} INFO - Running <TaskInstance: batch_pipeline.extract_database_tables scheduled__2023-03-20T00:00:00+00:00 [running]> on host 070f7bf9e1a1
[2023-03-21T15:53:08.838+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Pedrosa
AIRFLOW_CTX_DAG_ID=batch_pipeline
AIRFLOW_CTX_TASK_ID=extract_database_tables
AIRFLOW_CTX_EXECUTION_DATE=2023-03-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=4
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-20T00:00:00+00:00
[2023-03-21T15:53:08.882+0000] {logging_mixin.py:137} INFO - Erro: (psycopg2.OperationalError) connection to server at "host.docker.internal" (192.168.65.2), port 5432 failed: FATAL:  password authentication failed for user "postgres"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-03-21T15:53:08.886+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-03-21T15:53:08.933+0000] {taskinstance.py:1326} INFO - Marking task as SUCCESS. dag_id=batch_pipeline, task_id=extract_database_tables, execution_date=20230320T000000, start_date=20230321T155307, end_date=20230321T155308
[2023-03-21T15:53:09.798+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-03-21T15:53:10.101+0000] {taskinstance.py:2585} INFO - 1 downstream tasks scheduled from follow-on schedule check
